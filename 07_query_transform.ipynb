{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Query Transformations for Enhanced RAG Systems\n",
    "\n",
    "This notebook implements three query transformation techniques to enhance retrieval performance in RAG systems without relying on specialized libraries like LangChain. By modifying user queries, we can significantly improve the relevance and comprehensiveness of retrieved information.\n",
    "\n",
    "## Key Transformation Techniques\n",
    "\n",
    "1. **Query Rewriting**: Makes queries more specific and detailed for better search precision.\n",
    "2. **Step-back Prompting**: Generates broader queries to retrieve useful contextual information.\n",
    "3. **Sub-query Decomposition**: Breaks complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Query Transformation Techniques\n",
    "### 1. Query Rewriting\n",
    "This technique makes queries more specific and detailed to improve precision in retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for query rewriting\n",
    "    Returns:\n",
    "        str: The rewritten query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be rewritten\n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    Original query: {original_query}\n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the rewritten query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the rewritten query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Step-back Prompting\n",
    "This technique generates broader queries to retrieve contextual background information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Generates a more general 'step-back' query to retrieve broader context.\n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for step-back query generation\n",
    "    Returns:\n",
    "        str: The step-back query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be generalized\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
    "    Original query: {original_query}\n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the step-back query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the step-back query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-query Decomposition\n",
    "This technique breaks down complex queries into simpler components for comprehensive retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into simpler sub-queries.\n",
    "    Args:\n",
    "        original_query (str): The original complex query\n",
    "        num_subqueries (int): Number of sub-queries to generate\n",
    "        model (str): The model to use for query decomposition \n",
    "    Returns:\n",
    "        List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be decomposed\n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the sub-queries using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Process the response to extract sub-queries\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numbered queries using simple parsing\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # Remove the number and leading space\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Query Transformation Techniques\n",
    "Let's apply these techniques to an example query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "original_query = \"What are the impacts of AI on job automation and employment?\"\n",
    "\n",
    "# Apply query transformations\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "# Query Rewriting\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# Step-back Prompting\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# Sub-query Decomposition\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store\n",
    "To demonstrate how query transformations integrate with retrieval, let's implement a simple vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, use empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Compute cosine similarity between query vector and stored vector\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the corresponding text\n",
    "                \"metadata\": self.metadata[idx],  # Add the corresponding metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The input text for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float]: The embedding vector.\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # Otherwise, return all embeddings as a list of vectors\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    # Create embeddings for all chunks at once for efficiency\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add chunks to vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    Search using a transformed query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query\n",
    "        vector_store (SimpleVectorStore): Vector store to search\n",
    "        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Search results\n",
    "    \"\"\"\n",
    "    print(f\"Transformation type: {transformation_type}\")\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # Query rewriting\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"Rewritten query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with rewritten query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # Step-back prompting\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"Step-back query: {transformed_query}\")\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with step-back query\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # Sub-query decomposition\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"Decomposed into sub-queries:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "        \n",
    "        # Create embeddings for all sub-queries\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        \n",
    "        # Search with each sub-query and combine results\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # Get fewer results per sub-query\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # Remove duplicates (keep highest similarity score)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        # Sort by similarity and take top_k\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "        \n",
    "    else:\n",
    "        # Regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response with Transformed Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and retrieved context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Retrieved context\n",
    "        model (str): The model to use for response generation\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are a helpful AI assistant. Answer the user's question based only on the provided context. If you cannot find the answer in the context, state that you don't have enough information.\"\n",
    "    \n",
    "    # Define the user prompt with the context and query\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please provide a comprehensive answer based only on the context above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the generated response, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete RAG Pipeline with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    Run complete RAG pipeline with optional query transformation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, transformed query, context, and response\n",
    "    \"\"\"\n",
    "    # Process the document to create a vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Apply query transformation and search\n",
    "    if transformation_type:\n",
    "        # Perform search with transformed query\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # Perform regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "    \n",
    "    # Combine context from search results\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    # Generate response based on the query and combined context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the results including original query, transformation type, context, and response\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_responses(results, reference_answer, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Compare responses from different query transformation techniques.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from different transformation techniques\n",
    "        reference_answer (str): Reference answer for comparison\n",
    "        model (str): Model for evaluation\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. \n",
    "    Your task is to compare different responses generated using various query transformation techniques \n",
    "    and determine which technique produced the best response compared to the reference answer.\"\"\"\n",
    "    \n",
    "    # Prepare the comparison text with the reference answer and responses from each technique\n",
    "    comparison_text = f\"\"\"Reference Answer: {reference_answer}\\n\\n\"\"\"\n",
    "    \n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} Query Response:\\n{result['response']}\\n\\n\"\n",
    "    \n",
    "    # Define the user prompt with the comparison text\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "    \n",
    "    Compare the responses generated by different query transformation techniques to the reference answer.\n",
    "    \n",
    "    For each technique (original, rewrite, step_back, decompose):\n",
    "    1. Score the response from 1-10 based on accuracy, completeness, and relevance\n",
    "    2. Identify strengths and weaknesses\n",
    "    \n",
    "    Then rank the techniques from best to worst and explain which technique performed best overall and why.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the evaluation response using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Print the evaluation results\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate different transformation techniques for the same query.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): Query to evaluate\n",
    "        reference_answer (str): Optional reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    # Define the transformation techniques to evaluate\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "    \n",
    "    # Run RAG with each transformation technique\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n===== Running RAG with {type_name} query =====\")\n",
    "        \n",
    "        # Get the result for the current transformation type\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "        \n",
    "        # Print the response for the current transformation type\n",
    "        print(f\"Response with {type_name} query:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Compare results if a reference answer is provided\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Extract the reference answer from the validation data\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# pdf_path\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
